# KG-Paper

## 1 大规模跨语言知识图谱

### 2018 年

1. XLORE2: Large-scale Cross-lingual Knowledge Graph Construction and Application 
   * 项目地址：<https://xlore.org/>
   * 论文地址：[XLORE2: Large-scale Cross-lingual Knowledge Graph Construction and Application](https://direct.mit.edu/dint/article/1/1/77/9977/XLORE2-Large-scale-Cross-lingual-Knowledge-Graph)
   * 笔记链接：[「论文笔记」：XLORE2.md](./kg-sys/「论文笔记」：XLORE2.md)

## 2 实体链接

### 2021 年

1. autoregressive entity retrieval *(ICLR 2021)*
   * 论文地址：[https://arxiv.org/abs/2010.00904](https://arxiv.org/abs/2010.00904) 
   * 代码地址：[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)
   * 笔记链接：[autoregressive entity retrieval](./entity-linking/自回归实体检索.md)
2. CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata *(EACL 2021)*
   * 论文地址：[https://arxiv.org/abs/2101.09969](https://arxiv.org/abs/2101.09969) 
   * 代码地址：[https://github.com/ManojPrabhakar/CHOLAN](https://github.com/ManojPrabhakar/CHOLAN)
   * 笔记链接：[cholan a modular approach for neural entity linking on wikipedia and wikidata](./entity-linking/CHOLAN-一种基于Wikipedia和Wikidata的模块化实体链接方法.md)
3. Neural Entity Linking A Survey of Models Based on Deep Learning
   * 论文地址：[https://arxiv.org/abs/2006.00575](https://arxiv.org/abs/2006.00575)
   * 笔记链接：[Neural Entity Linking A Survey of Models Based on Deep Learning.md](./entity-linking/Neural-Entity-Linking-A-Survey-of-Models-Based-on-Deep-Learning.md)
4. Combining Word and Entity Embeddings for Entity Linking *(ESWC 2017)*
   * 论文地址：[https://perso.limsi.fr/bg/fichiers/2017/combining-word-entity-eswc2017.pdf](https://perso.limsi.fr/bg/fichiers/2017/combining-word-entity-eswc2017.pdf) 
   * 笔记链接：[Combining Word and Entity Embeddings for Entity Linking.md](./entity-linking/Combining-Word-and-Entity-Embeddings-for-Entity-Linking.md)

### 2020 年

1. Scalable Zero-shot Entity Linking with Dense Entity Retrieval (*EMNLP 2020)*
   * 论文地址：[https://arxiv.org/abs/1911.03814](https://arxiv.org/abs/1911.03814)  
   * 代码实现：[https://github.com/facebookresearch/BLINK](https://github.com/facebookresearch/BLINK)
   * 笔记链接：[ Scalable Zero-shot Entity Linking with Dense Entity Retrieval](./entity-linking/面向零样本学习的可扩展实体链接方法.md)

## 3 Method

1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
   * 论文地址：[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
   * 代码实现：[https://github.com/google-research/bert](https://github.com/google-research/bert)
   * 笔记链接：[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](./method/Bert.md)
