# 自回归实体检索

## AUTOREGRESSIVE ENTITY RETRIEVAL

原文链接：[https://arxiv.org/abs/2010.00904](https://arxiv.org/abs/2010.00904) [ICLR 2021]

代码及与训练模型：[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)

## ABSTRACT

对于像开放域知识问答和实体链接这样的知识密集型任务来说，检索出给定 query 中的实体是这些任务的基础。现有的实体检索的方法可以理解为：**为每个实体构造一个 Classifier，其权重向量由 entity 的 meta information（entity 的 desc、infobox、type 等信息） 编码得出，实体检索就是进行多类别分类。** 这样的方法导致了以下缺点：

* query 的上下文信息和实体的语义交互只是通过“点积”实现，缺少深层次、细粒度的交互。
* 当 entity 集合较大时，需要大量的内存来存储 entity 的编码向量。
* 训练时，负样本集合必须进行下采样，不采样直接计算 softMax 计算成本太高。

论文作者提出了 GENRE，该系统是第一个以自回归方式，根据上下文从左到右逐个标记地生成实体的唯一标识，并以此标识检索实体的系统。

该系统一定程度上解决了上述的问题：

* 自回归公式允许模型直接捕获上下文和 entity 之间的关系，有效地对两者进行交叉编码
* encoder- decoder 架构的参数与词汇表大小成正比而不是实体数量，因此对内存的需求减少
* 无需对负样本进行下采样，就能有效计算准确的 softmax loss。

## 1 简介

以往的分类模型将 entity identifier 作为一个原子标签，忽略了一个重要的事实：**我们能获取到明确、高度结构化、组合性的 entity name（即相应的 Wikipedia 文章的标题）**。例如 Wikipedia 中每个文章都有一个独一无二的标题，这个标题可能是主体名称、主题描述、能用于消歧的潜在信息。

![image-20210805143631024](https://i.loli.net/2021/08/06/k8oLeXcjdNZwHUp.png)

entity name 经常以可预测的规则的方式与 mention context 进行交互，如上图所示：

*entity name* 可能：a) 中包含一个可以用来辅助推断的类型说明；b) 由 mention context 中的 token 组成；c) 是mention 的翻译；d) 是 mention 的正确的别名；e) 需要存储在模型参数中的事实知识；f) 与 mention 相同；

这些特征表明：**输入的 context 可以被逐字翻译成独一无二的 entity name，而不是只能被归类在一大堆实体选项中**。

因此，论文作者提出了第一个生成式的实体检索器，使用基于 seq2seq 的自回归架构——GENRE（for *Generative ENtity REtrieval*）GENRE 选用基于 transformers 的预训练语言模型 BART 权重为模型基础，再训练生成 entity name，该架构被证明能够保留事实知识并且有一定的语言翻译能力，一般来说生成的不一定是有效的 entity name，为了解决这个问题 GENRE 使用了受限制的 decoding 策略，限制生成的名称位于预定义的候选集。对于GENRE 模型，没有使用出了 entity name 以外的 meta information，因此可以可以很方便的向实体集中添加新实体。

论文作者在实体消歧、end2end 实体链接、文档检索 三个任务上进行了测试，取得了巨大的成功。

## 2 问题描述

实体检索被形式化定义如下：

假设有一个实体集 $\varepsilon$​​ ，其中每个 entity 都是 KB 中的一个条目，检索问题可以被定义为：给定一个文本输入 $x$，模型返回 $\varepsilon$​ 中最与之相关的实体，同时假设每个实体 $e \in \varepsilon$​ 都有唯一的文本表示：token序列： $y$

## 3 模型方法

GENRE 为每个实体 e 使用自回归公式计算得分，并以此排序，自回归公式如下：
$$
score(e|x) = p_{\theta}(y|x) = \prod_{i=1}^{N} p_{\theta}(y_i |y_{< i}, x)
$$
其中 $y$​ 是 entity 的表示，为一个 N 个 token 的序列，$\theta$​ 是模型参数，作者采用 fine-turning BART 的方式，使用一个标准的 seq2seq 优化目标，最大化预测的输出序列的可能性。使用机器翻译的训练方法，将最大化 $log (p_{\theta}(y|x))$​​ ，通过因式分解，使得不需要进行负采样估算 Loss。

### 3.1 使用受限波束搜索进行推理 Constrained BS

如果为知识库中的每个 entity 都计算 score 效率就会很低，因此作者采用了**波束搜索**建立了一种近似的 decoder 策略，以有效的搜索实体空间。为了将生成的 entity name 是有效的，作者提出了受限波束搜索，以前缀树的形式定义了约束 $\tau$​。前缀树的每个 node 用词典中的 token 进行注释.

### 3.2 端到端的自回归实体链接

端到端的实体链接需要进行 mention detection，作者继续使用自回归的方式，生成带 Markup 的输入文本，其中 Markup 包括标记 mention 区域的`[]` ，标记生成的 entity 区域的`()`。不同于之前将生成的entity name 限制在预定义的实体集中，这次生成的结果是开放域的，因为可以输入任何语言，此时的前缀树 trie 会无限大，因此采用动态生成 trie 的策略。

![image-20210805170613829](https://i.loli.net/2021/08/06/yRZiKmNrQ3xe6uv.png)

如上图所示，经过 GENRE 可以直接生成带 mention detection 和 entity distribution 的结果。引入Markup之后，生成模型有三个阶段：

* 当前预测的词语不在 mention 或 entity 中，此时可以
  * 预测原句的下一个词
  * 开始一个新的 mention detection，即输出 Markup ：`[`
* 当前预测的词在 mention 中，此时可以：
  * 预测原句的下一词
  * 结束当前 mention 的检测，并开始检索相应的 entity。`]`,`(`
* 当前在 entity linking，此时可以：
  * 根据实体前缀树继续生成实体
  * 结束当前实体生成，即输出 `)`



